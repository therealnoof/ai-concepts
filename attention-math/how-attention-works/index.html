<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How Attention Math Works ‚Äî The Matrix Calculations Inside Every Transformer</title>

<!-- Font pairing: Plus Jakarta Sans for body, Space Mono for code/labels -->
<link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@300;400;500;600;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">

<style>
  /* ============================
     CSS VARIABLES
     Deep fuchsia/magenta theme ‚Äî rich purples with
     electric pink accents on a near-black background.
     This palette hasn't been used in any other page yet.
     ============================ */
  :root {
    --bg-deep:        #080610;
    --bg-panel:       #0f0c1a;
    --bg-card:        #150f24;
    --text-primary:   #ede8f5;
    --text-secondary: #9d90b8;
    --text-muted:     #5e5378;
    --accent-fuchsia: #e879f9;
    --accent-violet:  #c026d3;
    --accent-pink:    #f472b6;
    --accent-purple:  #a855f7;
    --accent-cyan:    #22d3ee;
    --accent-green:   #4ade80;
    --accent-amber:   #fbbf24;
    --accent-rose:    #fb7185;
    --glow-fuchsia:   rgba(232, 121, 249, 0.25);
    --glow-purple:    rgba(168, 85, 247, 0.2);
    --border-subtle:  rgba(157, 144, 184, 0.12);
    --border-fuchsia: rgba(232, 121, 249, 0.25);
  }

  /* ============================
     RESET & BASE
     ============================ */
  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    background: var(--bg-deep);
    color: var(--text-primary);
    font-family: 'Plus Jakarta Sans', sans-serif;
    min-height: 100vh;
    overflow-x: hidden;
  }

  /* ============================
     NAVIGATION BAR
     Matches the structure across all other pages.
     This page ("Attention Math") is added to the nav
     and links back to all the others.
     ============================ */
  .nav-bar {
    position: sticky;
    top: 0;
    z-index: 50;
    background: rgba(8, 6, 16, 0.88);
    backdrop-filter: blur(12px);
    -webkit-backdrop-filter: blur(12px);
    border-bottom: 1px solid var(--border-subtle);
    padding: 0 24px;
  }

  .nav-inner {
    max-width: 1200px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: space-between;
    height: 52px;
  }

  .nav-brand {
    font-family: 'Space Mono', monospace;
    font-size: 0.78rem;
    color: var(--accent-fuchsia);
    letter-spacing: 1.5px;
    text-transform: uppercase;
    opacity: 0.8;
    text-decoration: none;
    transition: opacity 0.2s ease;
  }
  .nav-brand:hover { opacity: 1; }

  .nav-links {
    display: flex;
    align-items: center;
    gap: 4px;
    flex-wrap: wrap;
  }

  .nav-link {
    font-family: 'Plus Jakarta Sans', sans-serif;
    font-size: 0.82rem;
    font-weight: 400;
    color: var(--text-secondary);
    text-decoration: none;
    padding: 5px 11px;
    border-radius: 8px;
    transition: all 0.2s ease;
  }
  .nav-link:hover {
    color: var(--text-primary);
    background: rgba(157, 144, 184, 0.08);
  }
  .nav-link.active {
    color: var(--accent-fuchsia);
    background: rgba(232, 121, 249, 0.08);
  }

  /* Small separator dot between nav links */
  .nav-sep {
    width: 3px; height: 3px;
    border-radius: 50%;
    background: var(--text-muted);
    opacity: 0.4;
    margin: 0 2px;
  }

  /* ============================
     MAIN CONTAINER
     ============================ */
  .container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 40px 24px;
  }

  /* ============================
     PAGE HEADER
     ============================ */
  .header {
    text-align: center;
    margin-bottom: 48px;
  }

  .subtitle {
    font-family: 'Space Mono', monospace;
    font-size: 0.72rem;
    color: var(--accent-fuchsia);
    letter-spacing: 3px;
    text-transform: uppercase;
    margin-bottom: 14px;
    opacity: 0.8;
  }

  .header h1 {
    font-size: 2.4rem;
    font-weight: 700;
    background: linear-gradient(135deg, var(--accent-fuchsia), var(--accent-pink), var(--accent-purple));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 14px;
    letter-spacing: -0.5px;
    line-height: 1.2;
  }

  .header p {
    color: var(--text-secondary);
    font-size: 1.05rem;
    font-weight: 300;
    max-width: 680px;
    margin: 0 auto;
    line-height: 1.65;
  }

  /* ============================
     SECTION TITLES
     Used above each major section of the page
     ============================ */
  .section-title {
    font-size: 1.4rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 8px;
  }
  .section-subtitle {
    color: var(--text-secondary);
    font-size: 0.95rem;
    font-weight: 300;
    margin-bottom: 28px;
    line-height: 1.6;
  }

  /* ============================
     CONCEPT CARDS ‚Äî Weights vs Params vs Dims
     Three side-by-side cards explaining the 
     foundational vocabulary before the math
     ============================ */
  .concept-grid {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 20px;
    margin-bottom: 48px;
  }
  @media (max-width: 768px) {
    .concept-grid { grid-template-columns: 1fr; }
  }

  .concept-card {
    background: var(--bg-card);
    border: 1px solid var(--border-subtle);
    border-radius: 16px;
    padding: 28px;
    position: relative;
    overflow: hidden;
    transition: all 0.3s ease;
  }
  .concept-card::before {
    /* Colored top border strip ‚Äî each card gets its own color */
    content: '';
    position: absolute;
    top: 0; left: 0; right: 0;
    height: 3px;
    border-radius: 16px 16px 0 0;
  }
  .concept-card:nth-child(1)::before { background: var(--accent-fuchsia); }
  .concept-card:nth-child(2)::before { background: var(--accent-purple); }
  .concept-card:nth-child(3)::before { background: var(--accent-cyan); }
  .concept-card:hover {
    border-color: rgba(232, 121, 249, 0.25);
    transform: translateY(-2px);
  }

  .concept-icon {
    font-size: 2rem;
    margin-bottom: 14px;
  }
  .concept-label {
    font-family: 'Space Mono', monospace;
    font-size: 0.7rem;
    color: var(--text-muted);
    letter-spacing: 2px;
    text-transform: uppercase;
    margin-bottom: 8px;
  }
  .concept-title {
    font-size: 1.25rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 12px;
  }
  .concept-desc {
    color: var(--text-secondary);
    font-size: 0.92rem;
    font-weight: 300;
    line-height: 1.65;
    margin-bottom: 14px;
  }
  .concept-example {
    background: rgba(255, 255, 255, 0.04);
    border: 1px solid var(--border-subtle);
    border-radius: 8px;
    padding: 10px 14px;
    font-family: 'Space Mono', monospace;
    font-size: 0.78rem;
    color: var(--accent-fuchsia);
    line-height: 1.6;
  }

  /* ============================
     DIAGRAM SECTION
     The main SVG container ‚Äî consistent style
     with the grid background dot pattern
     ============================ */
  .diagram-section {
    background: var(--bg-card);
    border: 1px solid var(--border-subtle);
    border-radius: 20px;
    padding: 32px 16px;
    margin-bottom: 40px;
    position: relative;
    overflow: hidden;
  }

  /* Subtle dot grid background pattern */
  .diagram-section::before {
    content: '';
    position: absolute;
    inset: 0;
    background-image:
      radial-gradient(circle at 1px 1px, rgba(232, 121, 249, 0.04) 1px, transparent 0);
    background-size: 40px 40px;
    pointer-events: none;
  }

  .diagram-svg {
    width: 100%;
    height: auto;
    display: block;
  }

  .diagram-label {
    text-align: center;
    font-family: 'Space Mono', monospace;
    font-size: 0.72rem;
    color: var(--text-muted);
    letter-spacing: 1.5px;
    text-transform: uppercase;
    margin-top: 16px;
  }

  /* ============================
     CONTROLS / BUTTONS
     Match the style from other pages in the site
     ============================ */
  .controls {
    text-align: center;
    margin-top: 20px;
    position: relative;
    z-index: 2;
    display: flex;
    justify-content: center;
    gap: 12px;
    flex-wrap: wrap;
  }

  .btn {
    font-family: 'Space Mono', monospace;
    font-size: 0.82rem;
    padding: 12px 24px;
    border-radius: 10px;
    cursor: pointer;
    transition: all 0.3s ease;
    letter-spacing: 0.3px;
    border: 1px solid;
  }
  .btn-primary {
    background: linear-gradient(135deg, rgba(232, 121, 249, 0.15), rgba(168, 85, 247, 0.15));
    border-color: rgba(232, 121, 249, 0.35);
    color: var(--accent-fuchsia);
  }
  .btn-primary:hover {
    background: linear-gradient(135deg, rgba(232, 121, 249, 0.25), rgba(168, 85, 247, 0.25));
    border-color: var(--accent-fuchsia);
    box-shadow: 0 0 20px rgba(232, 121, 249, 0.2);
  }
  .btn-secondary {
    background: rgba(255, 255, 255, 0.04);
    border-color: var(--border-subtle);
    color: var(--text-secondary);
  }
  .btn-secondary:hover {
    background: rgba(255, 255, 255, 0.07);
    color: var(--text-primary);
  }
  .btn:active { transform: scale(0.97); }

  /* Status bar that appears below the diagram during walkthrough */
  .status-bar {
    text-align: center;
    font-family: 'Space Mono', monospace;
    font-size: 0.8rem;
    color: var(--accent-fuchsia);
    min-height: 28px;
    margin-top: 12px;
    opacity: 0;
    transition: opacity 0.3s ease;
  }
  .status-bar.visible { opacity: 1; }

  /* ============================
     INTERACTIVE ATTENTION DEMO
     Shows a sentence where you click a word
     to see its attention heat map
     ============================ */
  .attention-panel {
    background: var(--bg-card);
    border: 1px solid var(--border-fuchsia);
    border-radius: 16px;
    padding: 28px;
    margin-bottom: 40px;
    display: none; /* Hidden until the button is clicked */
  }
  .attention-panel.visible { display: block; }

  .attention-header {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 8px;
  }
  .attention-instruction {
    color: var(--text-secondary);
    font-size: 0.9rem;
    margin-bottom: 20px;
  }

  /* The sentence with clickable words */
  .attention-sentence {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    margin-bottom: 24px;
  }

  /* Each word is a clickable chip */
  .attn-word {
    padding: 8px 16px;
    border-radius: 8px;
    border: 1px solid var(--border-subtle);
    background: rgba(255, 255, 255, 0.04);
    cursor: pointer;
    font-size: 1.05rem;
    font-weight: 500;
    transition: all 0.2s ease;
    position: relative;
  }
  .attn-word:hover {
    border-color: rgba(232, 121, 249, 0.4);
    color: var(--accent-fuchsia);
  }
  .attn-word.selected {
    border-color: var(--accent-fuchsia);
    background: rgba(232, 121, 249, 0.12);
    color: var(--accent-fuchsia);
  }
  /* The attention heat ‚Äî background color intensity shows attention weight */
  .attn-word.heat-high {
    background: rgba(232, 121, 249, 0.28);
    border-color: rgba(232, 121, 249, 0.6);
    color: var(--text-primary);
  }
  .attn-word.heat-mid {
    background: rgba(168, 85, 247, 0.16);
    border-color: rgba(168, 85, 247, 0.4);
    color: var(--text-primary);
  }
  .attn-word.heat-low {
    background: rgba(94, 83, 120, 0.12);
    border-color: var(--border-subtle);
    color: var(--text-secondary);
  }

  /* The score badge shown on each word */
  .attn-score {
    position: absolute;
    top: -10px;
    right: -4px;
    background: var(--accent-fuchsia);
    color: #080610;
    font-family: 'Space Mono', monospace;
    font-size: 0.65rem;
    font-weight: 700;
    padding: 2px 6px;
    border-radius: 6px;
    opacity: 0;
    transition: opacity 0.3s ease;
  }
  .attn-score.visible { opacity: 1; }

  .attention-explanation {
    background: rgba(255,255,255,0.03);
    border-left: 3px solid var(--accent-fuchsia);
    border-radius: 0 8px 8px 0;
    padding: 14px 18px;
    color: var(--text-secondary);
    font-size: 0.92rem;
    line-height: 1.6;
  }

  /* ============================
     MATRIX SHAPE EXPLAINER
     Shows the actual matrix dimensions
     for LLaMA 3 8B as a visual reference
     ============================ */
  .matrix-grid {
    display: grid;
    grid-template-columns: repeat(4, 1fr);
    gap: 16px;
    margin-bottom: 40px;
  }
  @media (max-width: 900px) {
    .matrix-grid { grid-template-columns: repeat(2, 1fr); }
  }
  @media (max-width: 480px) {
    .matrix-grid { grid-template-columns: 1fr; }
  }

  .matrix-card {
    background: var(--bg-card);
    border: 1px solid var(--border-subtle);
    border-radius: 14px;
    padding: 20px;
    transition: all 0.3s ease;
    text-align: center;
  }
  .matrix-card:hover {
    border-color: rgba(232, 121, 249, 0.3);
    transform: translateY(-2px);
  }
  .matrix-name {
    font-family: 'Space Mono', monospace;
    font-size: 1.1rem;
    font-weight: 700;
    margin-bottom: 8px;
  }
  .matrix-shape {
    font-family: 'Space Mono', monospace;
    font-size: 0.8rem;
    color: var(--accent-fuchsia);
    background: rgba(232, 121, 249, 0.08);
    border-radius: 6px;
    padding: 4px 10px;
    display: inline-block;
    margin-bottom: 10px;
  }
  .matrix-desc {
    font-size: 0.82rem;
    color: var(--text-secondary);
    line-height: 1.55;
  }

  /* ============================
     STEP CARDS
     Same pattern used across the whole site
     ============================ */
  .steps-grid {
    display: grid;
    grid-template-columns: 1fr;
    gap: 20px;
    margin-bottom: 40px;
  }
  @media (min-width: 768px) {
    .steps-grid { grid-template-columns: repeat(2, 1fr); }
  }

  .step-card {
    background: var(--bg-card);
    border: 1px solid var(--border-subtle);
    border-radius: 16px;
    padding: 28px;
    position: relative;
    transition: all 0.3s ease;
    overflow: hidden;
  }
  .step-card::before {
    content: '';
    position: absolute;
    top: 0; left: 0; right: 0;
    height: 3px;
    border-radius: 16px 16px 0 0;
  }
  .step-card:nth-child(1)::before  { background: var(--accent-fuchsia); }
  .step-card:nth-child(2)::before  { background: var(--accent-purple); }
  .step-card:nth-child(3)::before  { background: var(--accent-cyan); }
  .step-card:nth-child(4)::before  { background: var(--accent-green); }
  .step-card:nth-child(5)::before  { background: var(--accent-amber); }
  .step-card:nth-child(6)::before  { background: var(--accent-rose); }
  .step-card:nth-child(7)::before  { background: var(--accent-pink); }
  .step-card:nth-child(8)::before  { background: var(--accent-fuchsia); }
  .step-card:hover {
    border-color: rgba(232, 121, 249, 0.25);
    transform: translateY(-2px);
  }

  .step-num {
    font-family: 'Space Mono', monospace;
    font-size: 0.7rem;
    color: var(--text-muted);
    letter-spacing: 2px;
    text-transform: uppercase;
    margin-bottom: 10px;
  }
  .step-title {
    font-size: 1.15rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 10px;
  }
  .step-desc {
    color: var(--text-secondary);
    font-weight: 300;
    font-size: 0.93rem;
    line-height: 1.65;
  }
  .step-example {
    margin-top: 12px;
    padding: 10px 14px;
    background: rgba(255, 255, 255, 0.03);
    border-left: 2px solid var(--accent-fuchsia);
    border-radius: 0 6px 6px 0;
    font-family: 'Space Mono', monospace;
    font-size: 0.78rem;
    color: var(--accent-fuchsia);
    line-height: 1.6;
  }

  /* ============================
     LORA CALLOUT
     A highlighted box for the LoRA/fine-tuning
     connection at the bottom of the page
     ============================ */
  .callout {
    background: rgba(232, 121, 249, 0.06);
    border: 1px solid rgba(232, 121, 249, 0.2);
    border-radius: 16px;
    padding: 28px 32px;
    margin-bottom: 40px;
  }
  .callout-title {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--accent-fuchsia);
    margin-bottom: 10px;
  }
  .callout-body {
    color: var(--text-secondary);
    font-size: 0.95rem;
    font-weight: 300;
    line-height: 1.7;
  }
  .callout-body strong { color: var(--text-primary); font-weight: 600; }
  .callout-body code {
    font-family: 'Space Mono', monospace;
    font-size: 0.82rem;
    background: rgba(255,255,255,0.05);
    padding: 2px 6px;
    border-radius: 4px;
    color: var(--accent-fuchsia);
  }

  /* ============================
     TOOLTIP
     Appears when hovering over SVG nodes
     ============================ */
  .tooltip {
    position: fixed;
    z-index: 100;
    background: rgba(15, 12, 26, 0.97);
    border: 1px solid var(--border-fuchsia);
    border-radius: 10px;
    padding: 12px 16px;
    max-width: 260px;
    pointer-events: none;
    opacity: 0;
    transition: opacity 0.15s ease;
    box-shadow: 0 8px 32px rgba(0,0,0,0.5);
  }
  .tooltip.visible { opacity: 1; }
  .tooltip-title {
    font-weight: 600;
    font-size: 0.88rem;
    color: var(--accent-fuchsia);
    margin-bottom: 6px;
  }
  .tooltip-body {
    font-size: 0.82rem;
    color: var(--text-secondary);
    line-height: 1.55;
  }

  /* ============================
     NODE HIGHLIGHT ANIMATION
     Used during the walkthrough
     ============================ */
  @keyframes nodeGlow {
    0%   { opacity: 1; }
    40%  { opacity: 0.5; filter: drop-shadow(0 0 10px rgba(232, 121, 249, 0.8)); }
    100% { opacity: 1; }
  }
  .arch-node.active { animation: nodeGlow 0.8s ease-out; }

  @keyframes arrowFlash {
    0%   { stroke-opacity: 0.15; }
    40%  { stroke-opacity: 0.9; }
    100% { stroke-opacity: 0.15; }
  }
  .flow-arrow.active { animation: arrowFlash 0.7s ease-out; }

  /* ============================
     FOOTER
     ============================ */
  .footer {
    text-align: center;
    padding: 32px 0 16px;
    font-family: 'Space Mono', monospace;
    font-size: 0.72rem;
    color: var(--text-muted);
    letter-spacing: 0.5px;
    border-top: 1px solid var(--border-subtle);
  }

  /* ============================
     RESPONSIVE - smaller screens
     ============================ */
  @media (max-width: 640px) {
    .header h1 { font-size: 1.7rem; }
    .nav-links { gap: 2px; }
    .nav-link  { font-size: 0.7rem; padding: 4px 7px; }
    .nav-brand { font-size: 0.7rem; }
  }
</style>
</head>
<body>

<!-- ============================
     NAVIGATION BAR
     This page is "Attention Math" ‚Äî marked active.
     Matches the nav on all other pages exactly.
     ============================ -->
<nav class="nav-bar">
  <div class="nav-inner">
    <a href="/" class="nav-brand">‚ö° AI Concepts</a>
    <div class="nav-links">
      <a href="/" class="nav-link">Neural Networks</a>
      <div class="nav-sep"></div>
      <a href="/rag/how-rag-works" class="nav-link">RAG Pipeline</a>
      <div class="nav-sep"></div>
      <a href="/transformers/how-transformers-work" class="nav-link">Transformers</a>
      <div class="nav-sep"></div>
      <a href="/attention-math/how-attention-works" class="nav-link active">Attention Math</a>
      <div class="nav-sep"></div>
      <a href="/agentic-ai/how-agents-work" class="nav-link">Agentic AI</a>
      <div class="nav-sep"></div>
      <a href="/training/how-training-works" class="nav-link">Training Pipeline</a>
      <div class="nav-sep"></div>
      <a href="/ai-security/securing-ai" class="nav-link">AI Security</a>
      <div class="nav-sep"></div>
      <a href="/lab/lab-architecture" class="nav-link">Lab Architecture</a>
    </div>
  </div>
</nav>

<!-- Tooltip element ‚Äî populated by JS on SVG node hover -->
<div class="tooltip" id="tooltip">
  <div class="tooltip-title" id="tooltip-title"></div>
  <div class="tooltip-body" id="tooltip-body"></div>
</div>

<div class="container">

  <!-- ============================
       PAGE HEADER
       ============================ -->
  <div class="header">
    <div class="subtitle">Inside the Transformer</div>
    <h1>How Attention Math Works</h1>
    <p>Every time an LLM reads your prompt, billions of matrix multiplications happen in a specific sequence. This page walks through exactly what those calculations are, why they happen in that order, and how the numbers actually flow ‚Äî from raw text to a predicted next word.</p>
  </div>

  <!-- ============================
       SECTION 1: VOCABULARY
       Weights vs Parameters vs Dimensions
       ============================ -->
  <div class="section-title">üî§ The Vocabulary: Weights, Parameters, and Dimensions</div>
  <div class="section-subtitle">These three terms get used interchangeably but they mean different things. Get these straight before the math makes sense.</div>

  <div class="concept-grid">

    <!-- CARD 1: Parameters -->
    <div class="concept-card">
      <div class="concept-icon">üóÑÔ∏è</div>
      <div class="concept-label">Broadest term</div>
      <div class="concept-title">Parameters</div>
      <div class="concept-desc">
        Every number inside a model that was <strong>learned during training</strong>. When someone says "LLaMA 3 has 8 billion parameters," they mean 8,000,000,000 individual numbers are stored inside that model file. Parameters include weights and biases ‚Äî weights are ~99% of the total.
      </div>
      <div class="concept-example">
        LLaMA 3 8B ‚Üí 8,000,000,000 parameters<br>
        GPT-3 ‚Üí 175,000,000,000 parameters<br>
        GPT-4 (est.) ‚Üí ~1,760,000,000,000 parameters
      </div>
    </div>

    <!-- CARD 2: Weights -->
    <div class="concept-card">
      <div class="concept-icon">üéõÔ∏è</div>
      <div class="concept-label">Most of the parameters</div>
      <div class="concept-title">Weights</div>
      <div class="concept-desc">
        The specific numbers that control <strong>connection strength</strong> between neurons. Think of them as billions of volume knobs. Before training, they're random. After training on the entire internet, each is tuned to a precise value that ‚Äî together ‚Äî produces intelligent behavior.
      </div>
      <div class="concept-example">
        Parameters = Weights + Biases<br>
        Weights ‚âà 99% of total<br>
        Fine-tuning = re-tuning the knobs for your use case
      </div>
    </div>

    <!-- CARD 3: Dimensions -->
    <div class="concept-card">
      <div class="concept-icon">üìê</div>
      <div class="concept-label">The shape of data</div>
      <div class="concept-title">Dimensions</div>
      <div class="concept-desc">
        The <strong>length of the vectors</strong> flowing through the model. Every token gets converted to a vector of numbers ‚Äî the dimension is how many numbers long that list is. Higher dimensions encode richer meaning. LLaMA 3 8B uses 4,096 dimensions; GPT-4 likely uses 25,000+.
      </div>
      <div class="concept-example">
        "firewall" ‚Üí [0.23, -0.87, 0.41, ...] ‚Üê 4,096 numbers<br>
        BERT base: 768 dims ‚Üí simpler understanding<br>
        LLaMA 3 8B: 4,096 dims ‚Üí richer understanding
      </div>
    </div>

  </div>

  <!-- ============================
       SECTION 2: THE Q/K/V DIAGRAM
       Main SVG showing the matrix math flow
       ============================ -->
  <div class="section-title">üßÆ The Matrix Math Flow ‚Äî Step by Step</div>
  <div class="section-subtitle">Hover over each block for details. Hit "Walk Through" to see the data flow in sequence.</div>

  <div class="diagram-section">
    <!--
      SVG DIAGRAM: The Q, K, V Attention Calculation
      
      Shows the flow for a 5-token input sentence
      using LLaMA 3 8B dimensions as an example:
      
      [Input Matrix 5√ó4096]
             ‚îÇ
         √óWq   √óWk   √óWv     ‚Üê 3 weight matrix multiplications (per head)
             ‚îÇ
        [Q]   [K]   [V]       ‚Üê Query, Key, Value matrices (5√ó64 each)
             ‚îÇ
        Q √ó K·µÄ ‚Üí Scores       ‚Üê Dot product: how similar is each Q to each K?
             ‚îÇ
          Softmax              ‚Üê Convert scores to probabilities (sum to 1)
             ‚îÇ
        Scores √ó V ‚Üí Out      ‚Üê Weighted blend of Values
             ‚îÇ
        Concat 32 heads        ‚Üê All 32 heads combined (5√ó2048)
             ‚îÇ
        √óW_output              ‚Üê Project back to full dimension (5√ó4096)
             ‚îÇ
      [Feed Forward Network]   ‚Üê Process each token independently
             ‚îÇ
        [Next Layer / Output]
    -->
    <svg class="diagram-svg" viewBox="0 0 960 820" xmlns="http://www.w3.org/2000/svg" id="main-svg">

      <defs>
        <!-- Arrow markers in fuchsia and cyan -->
        <marker id="arr-fuchsia" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
          <polygon points="0 0, 8 3, 0 6" fill="#e879f9" opacity="0.7" />
        </marker>
        <marker id="arr-cyan" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
          <polygon points="0 0, 8 3, 0 6" fill="#22d3ee" opacity="0.7" />
        </marker>
        <marker id="arr-green" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
          <polygon points="0 0, 8 3, 0 6" fill="#4ade80" opacity="0.7" />
        </marker>
      </defs>

      <!-- ====== FLOW ARROWS ====== -->

      <!-- Input ‚Üí Wq/Wk/Wv split -->
      <line class="flow-arrow" id="fa-1"
        x1="480" y1="68" x2="480" y2="110"
        stroke="#e879f9" stroke-opacity="0.15" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />

      <!-- Fan out to three weight matrices -->
      <path class="flow-arrow" id="fa-2"
        d="M 480 136 L 240 175"
        fill="none" stroke="#e879f9" stroke-opacity="0.12" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />
      <line class="flow-arrow" id="fa-3"
        x1="480" y1="136" x2="480" y2="175"
        stroke="#e879f9" stroke-opacity="0.12" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />
      <path class="flow-arrow" id="fa-4"
        d="M 480 136 L 720 175"
        fill="none" stroke="#e879f9" stroke-opacity="0.12" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />

      <!-- Weight matrices ‚Üí Q, K, V results -->
      <line class="flow-arrow" id="fa-5"
        x1="240" y1="219" x2="240" y2="268"
        stroke="#a855f7" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />
      <line class="flow-arrow" id="fa-6"
        x1="480" y1="219" x2="480" y2="268"
        stroke="#a855f7" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />
      <line class="flow-arrow" id="fa-7"
        x1="720" y1="219" x2="720" y2="268"
        stroke="#a855f7" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />

      <!-- Q and K converge ‚Üí attention scores -->
      <path class="flow-arrow" id="fa-8"
        d="M 240 322 Q 240 370, 420 410"
        fill="none" stroke="#22d3ee" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-cyan)" />
      <path class="flow-arrow" id="fa-9"
        d="M 480 322 Q 480 370, 480 410"
        fill="none" stroke="#22d3ee" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-cyan)" />

      <!-- Scores ‚Üí Softmax -->
      <line class="flow-arrow" id="fa-10"
        x1="480" y1="460" x2="480" y2="498"
        stroke="#22d3ee" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-cyan)" />

      <!-- Softmax + V ‚Üí Weighted Output -->
      <path class="flow-arrow" id="fa-11"
        d="M 480 538 Q 480 570, 460 598"
        fill="none" stroke="#22d3ee" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-cyan)" />
      <path class="flow-arrow" id="fa-12"
        d="M 720 322 Q 720 570, 500 598"
        fill="none" stroke="#a855f7" stroke-opacity="0.12" stroke-width="1.5" marker-end="url(#arr-fuchsia)" />

      <!-- Output ‚Üí Feed Forward -->
      <line class="flow-arrow" id="fa-13"
        x1="480" y1="658" x2="480" y2="698"
        stroke="#4ade80" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-green)" />

      <!-- Feed Forward ‚Üí Next -->
      <line class="flow-arrow" id="fa-14"
        x1="480" y1="758" x2="480" y2="795"
        stroke="#4ade80" stroke-opacity="0.2" stroke-width="1.5" marker-end="url(#arr-green)" />

      <!-- ====== NODES ====== -->

      <!-- NODE 1: Input Matrix -->
      <g class="arch-node" id="node-input"
         data-tip-title="Input Matrix"
         data-tip-body="After tokenization and embedding lookup, each token in your prompt becomes a 4096-dimensional vector. For a 5-token prompt, this gives you a matrix shaped 5√ó4096 ‚Äî 5 rows (one per token), 4,096 columns (one per dimension). This is what flows into the attention block.">
        <rect x="330" y="22" width="300" height="48" rx="10"
          fill="rgba(232, 121, 249, 0.07)" stroke="rgba(232, 121, 249, 0.35)" stroke-width="1.5" />
        <text x="355" y="50" font-size="16">üì•</text>
        <text x="382" y="50" font-family="Plus Jakarta Sans" font-size="13" font-weight="600" fill="#ede8f5">Input Matrix</text>
        <text x="382" y="64" font-family="Space Mono" font-size="9" fill="#9d90b8">shape: [5 tokens √ó 4,096 dims]</text>
        <text x="598" y="64" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 1</text>
      </g>

      <!-- NODE 2: Split label -->
      <g id="node-split">
        <rect x="380" y="112" width="200" height="26" rx="8"
          fill="rgba(255,255,255,0.02)" stroke="rgba(157,144,184,0.15)" stroke-width="1" />
        <text x="480" y="129" font-family="Space Mono" font-size="9" fill="#5e5378" text-anchor="middle" letter-spacing="0.5">√ó 3 weight matrices simultaneously</text>
      </g>

      <!-- NODE 3a: Wq (Query weight matrix) -->
      <g class="arch-node" id="node-wq"
         data-tip-title="Wq ‚Äî Query Weight Matrix"
         data-tip-body="A learned matrix of shape [4096 √ó 64]. Multiplying your input by Wq produces the Query matrix ‚Äî the 'what am I looking for?' representation for each token. This matrix was trained to extract 'searching' features from each token's vector.">
        <rect x="140" y="175" width="200" height="46" rx="10"
          fill="rgba(232, 121, 249, 0.06)" stroke="rgba(232, 121, 249, 0.3)" stroke-width="1.5" />
        <text x="165" y="198" font-family="Space Mono" font-size="13" font-weight="700" fill="#e879f9">Wq</text>
        <text x="195" y="198" font-family="Plus Jakarta Sans" font-size="12" fill="#ede8f5">Query Weights</text>
        <text x="165" y="213" font-family="Space Mono" font-size="9" fill="#5e5378">[4,096 √ó 64]</text>
        <text x="295" y="213" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 2a</text>
      </g>

      <!-- NODE 3b: Wk (Key weight matrix) -->
      <g class="arch-node" id="node-wk"
         data-tip-title="Wk ‚Äî Key Weight Matrix"
         data-tip-body="A learned matrix of shape [4096 √ó 64]. Multiplying the input by Wk produces the Key matrix ‚Äî the 'what do I contain?' representation. During training, Wk learned to extract 'indexing' features so that related Queries and Keys produce high similarity scores.">
        <rect x="380" y="175" width="200" height="46" rx="10"
          fill="rgba(168, 85, 247, 0.06)" stroke="rgba(168, 85, 247, 0.3)" stroke-width="1.5" />
        <text x="405" y="198" font-family="Space Mono" font-size="13" font-weight="700" fill="#a855f7">Wk</text>
        <text x="435" y="198" font-family="Plus Jakarta Sans" font-size="12" fill="#ede8f5">Key Weights</text>
        <text x="405" y="213" font-family="Space Mono" font-size="9" fill="#5e5378">[4,096 √ó 64]</text>
        <text x="535" y="213" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 2b</text>
      </g>

      <!-- NODE 3c: Wv (Value weight matrix) -->
      <g class="arch-node" id="node-wv"
         data-tip-title="Wv ‚Äî Value Weight Matrix"
         data-tip-body="A learned matrix of shape [4096 √ó 64]. Multiplying the input by Wv produces the Value matrix ‚Äî the 'what information do I provide if I'm relevant?' representation. The Values are the actual content that gets mixed together based on attention scores.">
        <rect x="620" y="175" width="200" height="46" rx="10"
          fill="rgba(244, 114, 182, 0.06)" stroke="rgba(244, 114, 182, 0.3)" stroke-width="1.5" />
        <text x="645" y="198" font-family="Space Mono" font-size="13" font-weight="700" fill="#f472b6">Wv</text>
        <text x="675" y="198" font-family="Plus Jakarta Sans" font-size="12" fill="#ede8f5">Value Weights</text>
        <text x="645" y="213" font-family="Space Mono" font-size="9" fill="#5e5378">[4,096 √ó 64]</text>
        <text x="775" y="213" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 2c</text>
      </g>

      <!-- NODE 4a: Q result -->
      <g class="arch-node" id="node-q"
         data-tip-title="Q ‚Äî Query Matrix"
         data-tip-body="Result: [5 √ó 64]. Each of the 5 tokens now has a 64-number 'query' vector asking 'what information do I need from the other tokens?' The dimension shrinks from 4,096 to 64 because this is one attention head ‚Äî the 64 is called the head dimension.">
        <rect x="140" y="268" width="200" height="56" rx="10"
          fill="rgba(232, 121, 249, 0.08)" stroke="rgba(232, 121, 249, 0.4)" stroke-width="2" />
        <text x="165" y="291" font-family="Space Mono" font-size="14" font-weight="700" fill="#e879f9">Q</text>
        <text x="185" y="291" font-family="Plus Jakarta Sans" font-size="13" fill="#ede8f5">Query Matrix</text>
        <text x="165" y="307" font-family="Space Mono" font-size="9" fill="#9d90b8">"What am I looking for?"</text>
        <text x="165" y="318" font-family="Space Mono" font-size="9" fill="#5e5378">[5 √ó 64]</text>
      </g>

      <!-- NODE 4b: K result -->
      <g class="arch-node" id="node-k"
         data-tip-title="K ‚Äî Key Matrix"
         data-tip-body="Result: [5 √ó 64]. Each token has a 64-number 'key' vector saying 'here's what I contain.' When we compute Q √ó K·µÄ, we're measuring how well each Query matches each Key ‚Äî producing an attention score for every (query token, key token) pair.">
        <rect x="380" y="268" width="200" height="56" rx="10"
          fill="rgba(168, 85, 247, 0.08)" stroke="rgba(168, 85, 247, 0.4)" stroke-width="2" />
        <text x="405" y="291" font-family="Space Mono" font-size="14" font-weight="700" fill="#a855f7">K</text>
        <text x="425" y="291" font-family="Plus Jakarta Sans" font-size="13" fill="#ede8f5">Key Matrix</text>
        <text x="405" y="307" font-family="Space Mono" font-size="9" fill="#9d90b8">"What do I contain?"</text>
        <text x="405" y="318" font-family="Space Mono" font-size="9" fill="#5e5378">[5 √ó 64]</text>
      </g>

      <!-- NODE 4c: V result -->
      <g class="arch-node" id="node-v"
         data-tip-title="V ‚Äî Value Matrix"
         data-tip-body="Result: [5 √ó 64]. Each token has a 64-number 'value' vector ‚Äî the actual content it contributes. The final step multiplies the attention probability scores by V to produce a weighted blend: tokens that got high attention scores contribute more of their value to the output.">
        <rect x="620" y="268" width="200" height="56" rx="10"
          fill="rgba(244, 114, 182, 0.08)" stroke="rgba(244, 114, 182, 0.4)" stroke-width="2" />
        <text x="645" y="291" font-family="Space Mono" font-size="14" font-weight="700" fill="#f472b6">V</text>
        <text x="665" y="291" font-family="Plus Jakarta Sans" font-size="13" fill="#ede8f5">Value Matrix</text>
        <text x="645" y="307" font-family="Space Mono" font-size="9" fill="#9d90b8">"What do I provide?"</text>
        <text x="645" y="318" font-family="Space Mono" font-size="9" fill="#5e5378">[5 √ó 64]</text>
      </g>

      <!-- NODE 5: Attention Scores (Q √ó K·µÄ) -->
      <g class="arch-node" id="node-scores"
         data-tip-title="Attention Scores ‚Äî Q √ó K·µÄ"
         data-tip-body="Q [5√ó64] √ó K·µÄ [64√ó5] = Scores [5√ó5]. The 5√ó5 result has one score per (query token, key token) pair. A high score means 'this query should pay a lot of attention to this key.' The scores are then divided by ‚àö64=8 for numerical stability ‚Äî called scaled dot-product attention.">
        <rect x="330" y="410" width="300" height="52" rx="10"
          fill="rgba(34, 211, 238, 0.07)" stroke="rgba(34, 211, 238, 0.35)" stroke-width="1.5" />
        <text x="355" y="430" font-size="15">‚úñÔ∏è</text>
        <text x="380" y="432" font-family="Plus Jakarta Sans" font-size="13" font-weight="600" fill="#ede8f5">Attention Scores  (Q √ó K·µÄ)</text>
        <text x="380" y="452" font-family="Space Mono" font-size="9" fill="#9d90b8">[5√ó64] √ó [64√ó5] = [5√ó5] matrix</text>
        <text x="595" y="452" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 3</text>
      </g>

      <!-- NODE 6: Softmax -->
      <g class="arch-node" id="node-softmax"
         data-tip-title="Scale + Softmax"
         data-tip-body="First, divide all scores by ‚àöhead_dim (‚àö64 = 8) to prevent scores from growing too large. Then apply softmax row-by-row ‚Äî this converts raw scores into probabilities that sum to 1.0 per row. Now each token has a probability distribution over all other tokens: 'pay 38% attention here, 14% there...'">
        <rect x="355" y="498" width="250" height="42" rx="10"
          fill="rgba(34, 211, 238, 0.05)" stroke="rgba(34, 211, 238, 0.25)" stroke-width="1.5" />
        <text x="380" y="519" font-size="14">üìä</text>
        <text x="404" y="521" font-family="Plus Jakarta Sans" font-size="13" font-weight="500" fill="#ede8f5">Scale √∑‚àö64 + Softmax</text>
        <text x="380" y="534" font-family="Space Mono" font-size="9" fill="#5e5378">scores ‚Üí probabilities (row sums = 1.0)</text>
      </g>

      <!-- NODE 7: Weighted Output (Scores √ó V) -->
      <g class="arch-node" id="node-weighted"
         data-tip-title="Weighted Output ‚Äî Scores √ó V"
         data-tip-body="Attention probabilities [5√ó5] √ó Values [5√ó64] = Output [5√ó64]. Each token's output vector is now a weighted blend of all Value vectors. A token that paid 38% attention to 'firewall' gets 38% of the firewall token's value mixed in. This is how context flows between tokens.">
        <rect x="310" y="598" width="340" height="62" rx="12"
          fill="rgba(168, 85, 247, 0.08)" stroke="rgba(168, 85, 247, 0.4)" stroke-width="2" />
        <text x="335" y="620" font-size="16">‚≠ê</text>
        <text x="362" y="622" font-family="Plus Jakarta Sans" font-size="14" font-weight="700" fill="#ede8f5">Attention Output</text>
        <text x="362" y="640" font-family="Space Mono" font-size="9" fill="#9d90b8">Scores [5√ó5] √ó V [5√ó64] = Output [5√ó64]</text>
        <text x="362" y="652" font-family="Space Mono" font-size="8" fill="#5e5378">each token = weighted blend of all values</text>
        <text x="610" y="652" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 4</text>
      </g>

      <!-- Multi-head annotation callout -->
      <g>
        <rect x="40" y="420" width="200" height="70" rx="8"
          fill="none" stroke="rgba(168, 85, 247, 0.2)" stroke-width="1" stroke-dasharray="4 3" />
        <text x="140" y="445" font-family="Space Mono" font-size="9" fill="#a855f7" text-anchor="middle">‚úï 32 HEADS</text>
        <text x="140" y="461" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">Steps 2-4 run 32 times</text>
        <text x="140" y="474" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">in parallel ‚Äî each head</text>
        <text x="140" y="487" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">learns different patterns</text>
        <line x1="240" y1="455" x2="330" y2="435" stroke="rgba(168, 85, 247, 0.15)" stroke-width="1" stroke-dasharray="4 3" />
      </g>

      <!-- NODE 8: Feed Forward -->
      <g class="arch-node" id="node-ffn"
         data-tip-title="Feed-Forward Network"
         data-tip-body="After the 32 heads are concatenated and projected back to [5√ó4096], each token passes through a small 2-layer neural network independently. In LLaMA 3 8B this expands to 11,008 dims then contracts back to 4,096. No cross-token communication here ‚Äî this is where factual knowledge is stored in the weights.">
        <rect x="310" y="698" width="340" height="62" rx="10"
          fill="rgba(74, 222, 128, 0.06)" stroke="rgba(74, 222, 128, 0.3)" stroke-width="1.5" />
        <text x="335" y="726" font-size="16">üßÆ</text>
        <text x="362" y="724" font-family="Plus Jakarta Sans" font-size="13" font-weight="600" fill="#ede8f5">Feed-Forward Network</text>
        <text x="362" y="742" font-family="Space Mono" font-size="9" fill="#5e5378">expand [5√ó4096]‚Üí[5√ó11008] ‚Üí contract [5√ó4096]</text>
        <text x="610" y="742" font-family="Space Mono" font-size="8" fill="#5e5378" letter-spacing="1">STEP 5</text>
      </g>

      <!-- NODE 9: Repeat label -->
      <g id="node-repeat">
        <rect x="330" y="792" width="300" height="26" rx="8"
          fill="rgba(255,255,255,0.02)" stroke="rgba(74, 222, 128, 0.2)" stroke-width="1" />
        <text x="480" y="809" font-family="Space Mono" font-size="9" fill="#4ade80" text-anchor="middle" letter-spacing="0.5">‚Üª repeat √ó 32 layers ‚Üí output projection</text>
      </g>

      <!-- Layer count callout on right -->
      <g>
        <rect x="725" y="698" width="170" height="62" rx="8"
          fill="none" stroke="rgba(74, 222, 128, 0.15)" stroke-width="1" stroke-dasharray="4 3" />
        <text x="810" y="718" font-family="Space Mono" font-size="9" fill="#4ade80" text-anchor="middle">32 LAYERS</text>
        <text x="810" y="734" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">Each full attention +</text>
        <text x="810" y="747" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">FFN block = 1 layer</text>
        <text x="810" y="756" font-family="Space Mono" font-size="8" fill="#5e5378" text-anchor="middle">LLaMA 3 8B has 32</text>
        <line x1="725" y1="729" x2="650" y2="729" stroke="rgba(74, 222, 128, 0.15)" stroke-width="1" stroke-dasharray="4 3" />
      </g>

    </svg>

    <div class="controls">
      <button class="btn btn-primary" id="btn-walk" onclick="walkthrough()">‚ñ∂ Walk Through the Math</button>
      <button class="btn btn-secondary" id="btn-attn" onclick="toggleAttentionDemo()">üîç Interactive Attention Demo</button>
    </div>
    <div class="status-bar" id="status-bar"></div>
  </div>

  <!-- ============================
       SECTION 3: INTERACTIVE ATTENTION DEMO
       Click a word to see its attention heat map
       ============================ -->
  <div class="attention-panel" id="attention-panel">
    <div class="attention-header">üîç Self-Attention Demo ‚Äî Click a word to see what it focuses on</div>
    <div class="attention-instruction">
      Each word in a sentence pays different amounts of attention to every other word. Click any word below to see its attention pattern ‚Äî this simulates what a trained model might learn.
    </div>
    <div class="attention-sentence" id="attn-sentence">
      <!-- Words injected by JavaScript below -->
    </div>
    <div class="attention-explanation" id="attn-explanation">
      Click any word above to see which other tokens it "pays attention to" when building its contextual understanding.
    </div>
  </div>

  <!-- ============================
       SECTION 4: MATRIX SHAPES REFERENCE
       Real dimension numbers for LLaMA 3 8B
       ============================ -->
  <div class="section-title">üìê Matrix Shapes in LLaMA 3 8B</div>
  <div class="section-subtitle">These are the actual numbers flowing through the model on every forward pass. The matrix dimensions tell you exactly how much computation is happening.</div>

  <div class="matrix-grid">

    <div class="matrix-card">
      <div class="matrix-name" style="color: var(--accent-fuchsia)">Input</div>
      <div class="matrix-shape">[tokens √ó 4,096]</div>
      <div class="matrix-desc">One 4,096-dim vector per input token. For a 100-token prompt this is a 100√ó4,096 matrix entering the first layer.</div>
    </div>

    <div class="matrix-card">
      <div class="matrix-name" style="color: var(--accent-purple)">Wq / Wk / Wv</div>
      <div class="matrix-shape">[4,096 √ó 128]</div>
      <div class="matrix-desc">Each attention head uses 128-dim Q/K/V. LLaMA 3 8B has 32 heads √ó 128 = 4,096 total. Three of these weight matrices exist per head.</div>
    </div>

    <div class="matrix-card">
      <div class="matrix-name" style="color: var(--accent-cyan)">Q √ó K·µÄ</div>
      <div class="matrix-shape">[tokens √ó tokens]</div>
      <div class="matrix-desc">Attention score matrix. For a 100-token prompt: 100√ó100 = 10,000 scores per head √ó 32 heads. This is why long context is expensive ‚Äî it grows quadratically.</div>
    </div>

    <div class="matrix-card">
      <div class="matrix-name" style="color: var(--accent-green)">Feed-Forward</div>
      <div class="matrix-shape">[4,096 ‚Üí 11,008]</div>
      <div class="matrix-desc">Expands each token vector to 11,008 dims, applies activation, then contracts back to 4,096. The expansion ratio stores factual knowledge.</div>
    </div>

  </div>

  <!-- ============================
       SECTION 5: STEP CARDS
       Detailed explanation of each math step
       ============================ -->
  <div class="section-title">üìã Step-by-Step Breakdown</div>
  <div class="section-subtitle">What's actually happening at each stage and why it was designed that way.</div>

  <div class="steps-grid">

    <div class="step-card">
      <div class="step-num">Step 01</div>
      <div class="step-title">üì• Input Embedding</div>
      <div class="step-desc">
        Before any attention math, each token ID is converted to a dense vector via an <strong>embedding lookup</strong> ‚Äî the first matrix operation. The embedding matrix has one row per vocabulary token (32,000 rows for LLaMA) and 4,096 columns. Your token ID is the row number you look up.
      </div>
      <div class="step-example">
        Token ID 8923 ‚Üí look up row 8923 ‚Üí [0.23, -0.11, 0.87, ...] (4,096 numbers)
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 02</div>
      <div class="step-title">‚úñÔ∏è Create Q, K, V</div>
      <div class="step-desc">
        The input matrix multiplies three separate learned weight matrices (Wq, Wk, Wv) simultaneously. Each produces a different "view" of the same input ‚Äî Query captures what the token is searching for, Key captures what it contains, Value captures what it contributes when selected.
      </div>
      <div class="step-example">
        [5√ó4096] √ó [4096√ó128] = [5√ó128] ‚Äî three times, giving Q, K, and V
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 03</div>
      <div class="step-title">üìê Score: Q √ó K·µÄ</div>
      <div class="step-desc">
        Every Query vector is compared against every Key vector using a <strong>dot product</strong> ‚Äî a single number measuring similarity. Higher score = more relevant. The result is a square matrix where row i, column j tells you "how much should token i attend to token j?"
      </div>
      <div class="step-example">
        "firewall" Query vs "attack" Key ‚Üí high score (learned to be related)<br>
        "firewall" Query vs "the" Key ‚Üí low score (not relevant)
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 04</div>
      <div class="step-title">üìä Scale + Softmax</div>
      <div class="step-desc">
        Scores are divided by ‚àöhead_dimension to prevent numerically unstable large values (this is called <strong>scaled</strong> dot-product attention). Then softmax converts each row to probabilities summing to 1.0 ‚Äî creating a clean probability distribution per token over all other tokens.
      </div>
      <div class="step-example">
        Row for "firewall": [0.1, 0.9, 0.3, 0.1, 0.6] ‚Üí after softmax ‚Üí [0.07, 0.38, 0.14, 0.07, 0.34]
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 05</div>
      <div class="step-title">üîÄ Weighted Blend: Scores √ó V</div>
      <div class="step-desc">
        Attention probabilities multiply the Value matrix. Each token's output is a <strong>weighted average of all Value vectors</strong>, weighted by how much it attended to each token. "Firewall" paid 34% attention to "attack," so 34% of the attack token's Value gets mixed into the firewall output.
      </div>
      <div class="step-example">
        Probs [5√ó5] √ó V [5√ó128] = Out [5√ó128] ‚Äî context now embedded in each token
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 06</div>
      <div class="step-title">üîÅ Multi-Head: Run 32 Times</div>
      <div class="step-desc">
        Steps 2‚Äì5 run in parallel across <strong>32 independent attention heads</strong>, each with its own Wq/Wk/Wv weights. Each head specializes in a different type of relationship ‚Äî syntax, semantics, coreference, proximity. All 32 outputs are concatenated (5√ó4,096) and projected back.
      </div>
      <div class="step-example">
        32 heads √ó 128 dims = 4,096 ‚Üí one weight matrix projects back to [5√ó4,096]
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 07</div>
      <div class="step-title">üßÆ Feed-Forward Network</div>
      <div class="step-desc">
        Each token independently passes through a 2-layer network: expand to 11,008 dims, apply a non-linear <strong>activation function</strong> (SiLU in LLaMA), contract back to 4,096. No cross-token communication here. This is where factual knowledge ‚Äî not contextual relationships ‚Äî is stored in the weights.
      </div>
      <div class="step-example">
        Attention = "which tokens relate to which"<br>Feed-forward = "what do I know about this enriched token"
      </div>
    </div>

    <div class="step-card">
      <div class="step-num">Step 08</div>
      <div class="step-title">üîÇ Repeat √ó 32 Layers</div>
      <div class="step-desc">
        Steps 2‚Äì7 are one "transformer layer." LLaMA 3 8B stacks 32 of these. Each layer's output feeds into the next. Early layers capture syntax and basic word relationships. Mid layers learn semantics. Deep layers handle abstract reasoning and complex inference ‚Äî each building on the previous.
      </div>
      <div class="step-example">
        Layer 1: "firewall" is a noun near "blocked"<br>
        Layer 16: "firewall" ‚Üí blocked an attack ‚Üí security event<br>
        Layer 32: confident next token prediction
      </div>
    </div>

  </div>

  <!-- ============================
       SECTION 6: LORA CALLOUT
       Connects everything to fine-tuning context
       ============================ -->
  <div class="callout">
    <div class="callout-title">üîß Why This Matters for Fine-Tuning (LoRA)</div>
    <div class="callout-body">
      Now that you can see the actual matrix shapes, LoRA (Low-Rank Adaptation) makes intuitive sense. 
      Full fine-tuning would update the entire <strong>Wq, Wk, Wv, and feed-forward weight matrices</strong> ‚Äî 
      for LLaMA 3 8B that's billions of numbers to update, requiring enormous GPU memory.
      <br><br>
      LoRA instead <strong>freezes all the original weight matrices</strong> and inserts two tiny 
      "adapter" matrices alongside Wq and Wk. Instead of updating a <code>[4096 √ó 128]</code> matrix 
      (524,288 numbers), LoRA uses two matrices: <code>[4096 √ó r]</code> and <code>[r √ó 128]</code> 
      where <code>r</code> is the rank ‚Äî typically 8 or 16. At rank 8 that's only 
      <code>4096√ó8 + 8√ó128 = 33,792 numbers</code> ‚Äî 94% fewer updates.
      <br><br>
      The math still flows through the same Q/K/V path ‚Äî you're just adding a small learned 
      adjustment to the frozen original weights rather than replacing them. This is why LoRA 
      fine-tuning can run on a single GPU in a lab environment.
    </div>
  </div>

  <!-- ============================
       SECTION 7: RAG CONNECTION
       How sentence transformers use the same math
       ============================ -->
  <div class="callout" style="border-color: rgba(34, 211, 238, 0.2); background: rgba(34, 211, 238, 0.04);">
    <div class="callout-title" style="color: var(--accent-cyan)">üîó Connection to RAG: The Same Math, Different Goal</div>
    <div class="callout-body">
      The sentence transformer in your LangChain RAG pipeline runs <strong>the exact same Q/K/V matrix 
      math</strong> described above ‚Äî just with smaller dimensions (384 instead of 4,096) and only 
      6 layers instead of 32. The key difference is what happens at the end.
      <br><br>
      A generative LLM takes the final token's vector and projects it to vocabulary scores to predict 
      the next word. A sentence transformer instead <strong>averages all token vectors into one 
      single vector</strong> (called pooling) ‚Äî producing one 384-number embedding that represents 
      the meaning of the entire chunk. That number is what gets stored in your vector database.
      <br><br>
      When a user query comes in, the same 6 layers of attention math run on the query, produce 
      another 384-number embedding, and the vector database finds stored embeddings with the smallest 
      angular distance (cosine similarity) ‚Äî which is just another dot product under the hood. 
      <strong>The whole RAG retrieval pipeline is matrix multiplication from start to finish.</strong>
    </div>
  </div>

  <div class="footer">
    Interactive Attention Math Diagram ‚Äî Hover over blocks for dimension details ¬∑ Walk through the Q/K/V calculation ¬∑ Try the attention demo
  </div>

</div>

<!-- ============================
     JAVASCRIPT
     ============================ -->
<script>

  // ============================
  // TOOLTIP LOGIC
  // Appears when hovering over any element with
  // data-tip-title and data-tip-body attributes
  // ============================
  const tooltipEl    = document.getElementById('tooltip');
  const tooltipTitle = document.getElementById('tooltip-title');
  const tooltipBody  = document.getElementById('tooltip-body');

  document.querySelectorAll('.arch-node').forEach(node => {
    node.addEventListener('mouseenter', () => {
      tooltipTitle.textContent = node.getAttribute('data-tip-title');
      tooltipBody.textContent  = node.getAttribute('data-tip-body');
      tooltipEl.classList.add('visible');
    });
    node.addEventListener('mousemove', (e) => {
      // Flip tooltip to left side if it would go off-screen right
          // Fully clamped tooltip positioning ‚Äî works on all screen sizes including mobile.
    // Tries right of cursor first, flips left if no room, then hard-clamps both edges.
    const tooltipWidth  = tooltipEl.offsetWidth  || 260;
    const tooltipHeight = tooltipEl.offsetHeight || 120;
    const viewportWidth  = window.innerWidth;
    const viewportHeight = window.innerHeight;
    const MARGIN = 12;

    // Horizontal: try right of cursor, flip left if it would overflow right edge
    let left = e.clientX + 16;
    if (left + tooltipWidth + MARGIN > viewportWidth) {
      left = e.clientX - tooltipWidth - 16;
    }
    // Hard clamp: never go off left OR right edge
    left = Math.max(MARGIN, Math.min(left, viewportWidth - tooltipWidth - MARGIN));

    // Vertical: position above cursor, clamp if it would go off bottom or top
    let top = e.clientY - 10;
    if (top + tooltipHeight + MARGIN > viewportHeight) {
      top = viewportHeight - tooltipHeight - MARGIN;
    }
    top = Math.max(MARGIN, top);

    tooltipEl.style.left = left + 'px';
    tooltipEl.style.top  = top  + 'px';
    });
    node.addEventListener('mouseleave', () => {
      tooltipEl.classList.remove('visible');
    });
  });


  // ============================
  // WALKTHROUGH ANIMATION
  // Steps through each node in the diagram
  // sequentially with a small delay between steps
  // ============================
  let isWalking = false;

  // Each step: which SVG node to highlight, which arrows to flash, status message
  const walkthroughSteps = [
    {
      nodeId:   'node-input',
      arrowIds: ['fa-1'],
      status:   'üì• Step 1: Input matrix ‚Äî tokens converted to 4,096-dim vectors...'
    },
    {
      nodeId:   'node-wq',
      arrowIds: ['fa-2'],
      status:   '‚úñÔ∏è Step 2a: Input √ó Wq ‚Üí Query matrix [5√ó128] ‚Äî "what am I searching for?"'
    },
    {
      nodeId:   'node-wk',
      arrowIds: ['fa-3'],
      status:   '‚úñÔ∏è Step 2b: Input √ó Wk ‚Üí Key matrix [5√ó128] ‚Äî "what do I contain?"'
    },
    {
      nodeId:   'node-wv',
      arrowIds: ['fa-4'],
      status:   '‚úñÔ∏è Step 2c: Input √ó Wv ‚Üí Value matrix [5√ó128] ‚Äî "what info do I provide?"'
    },
    {
      nodeId:   'node-q',
      arrowIds: ['fa-5'],
      status:   'üü£ Query matrix ready ‚Äî each token has a 128-number search vector...'
    },
    {
      nodeId:   'node-k',
      arrowIds: ['fa-6'],
      status:   'üü£ Key matrix ready ‚Äî each token has a 128-number index vector...'
    },
    {
      nodeId:   'node-v',
      arrowIds: ['fa-7'],
      status:   'üü£ Value matrix ready ‚Äî each token has a 128-number content vector...'
    },
    {
      nodeId:   'node-scores',
      arrowIds: ['fa-8', 'fa-9'],
      status:   'üìê Step 3: Q √ó K·µÄ ‚Üí 5√ó5 attention scores ‚Äî how much should each token attend to each other?'
    },
    {
      nodeId:   'node-softmax',
      arrowIds: ['fa-10'],
      status:   'üìä Scale √∑‚àö128, then softmax ‚Üí scores become probabilities summing to 1.0...'
    },
    {
      nodeId:   'node-weighted',
      arrowIds: ['fa-11', 'fa-12'],
      status:   '‚≠ê Step 4: Scores √ó V ‚Üí weighted output ‚Äî each token enriched with context!'
    },
    {
      nodeId:   'node-ffn',
      arrowIds: ['fa-13'],
      status:   'üßÆ Step 5: Feed-Forward ‚Äî each token processed independently through 2-layer network...'
    },
    {
      nodeId:   'node-repeat',
      arrowIds: ['fa-14'],
      status:   'üîÇ One layer complete. LLaMA 3 8B repeats this 32 times ‚Üí final token prediction!'
    }
  ];

  function walkthrough() {
    if (isWalking) return;
    isWalking = true;

    const btn    = document.getElementById('btn-walk');
    const status = document.getElementById('status-bar');
    btn.textContent  = '‚è≥ Walking through...';
    btn.style.opacity = '0.6';
    status.classList.add('visible');

    walkthroughSteps.forEach((step, idx) => {
      setTimeout(() => {
        // Update status text
        status.textContent = step.status;

        // Highlight the node
        const node = document.getElementById(step.nodeId);
        if (node) {
          node.classList.add('active');
          setTimeout(() => node.classList.remove('active'), 900);
        }

        // Flash the arrows
        step.arrowIds.forEach(id => {
          const arrow = document.getElementById(id);
          if (arrow) {
            arrow.classList.add('active');
            setTimeout(() => arrow.classList.remove('active'), 800);
          }
        });

        // Re-enable button after last step
        if (idx === walkthroughSteps.length - 1) {
          setTimeout(() => {
            btn.textContent  = '‚ñ∂ Walk Through the Math';
            btn.style.opacity = '1';
            isWalking = false;
          }, 1200);
        }
      }, idx * 1100); // 1.1 second between each step
    });
  }


  // ============================
  // INTERACTIVE ATTENTION DEMO
  // Simulates what a trained model's attention
  // might look like for a cybersecurity sentence.
  //
  // The attention weights here are hand-crafted
  // to reflect realistic patterns (e.g. "firewall"
  // attends strongly to "blocked" and "attack").
  // ============================

  // The demo sentence
  const demoWords = [
    "The", "firewall", "blocked", "the", "attack", "successfully"
  ];

  // Pre-defined attention patterns per word.
  // Values represent how much each word attends to every other word.
  // Rows = the selected word (query), Columns = all words (keys).
  // These simulate realistic learned attention patterns.
  const attentionData = {
    "The":          [0.55, 0.05, 0.05, 0.25, 0.05, 0.05],
    "firewall":     [0.05, 0.65, 0.30, 0.05, 0.55, 0.15],
    "blocked":      [0.05, 0.50, 0.70, 0.05, 0.60, 0.25],
    "the":          [0.40, 0.05, 0.05, 0.55, 0.05, 0.05],
    "attack":       [0.05, 0.60, 0.45, 0.05, 0.70, 0.10],
    "successfully": [0.05, 0.30, 0.65, 0.05, 0.40, 0.50]
  };

  // Explanations shown when each word is selected
  const attentionExplanations = {
    "The":          "\"The\" is a determiner ‚Äî it attends mostly to itself and the other \"the\" (its grammatical partner), with low attention to content words. Determiners carry positional/grammatical attention patterns.",
    "firewall":     "\"firewall\" is the subject and a key concept ‚Äî it attends strongly to itself, \"blocked\" (its verb), and \"attack\" (the object it acted on). This reflects the semantic relationship: firewall ‚Üí blocked ‚Üí attack.",
    "blocked":      "\"blocked\" is the main verb ‚Äî it attends to both \"firewall\" (its subject) and \"attack\" (its object) with high scores, and \"successfully\" (the adverb modifying it). Verbs tend to have broad attention spanning the clause.",
    "the":          "The second \"the\" is a determiner for \"attack\" ‚Äî it attends mostly to the first \"the\" (similar grammatical role) and itself. Low semantic content means low attention to content words.",
    "attack":       "\"attack\" is the object of the action ‚Äî it attends strongly to itself, \"blocked\" (the action taken against it), and \"firewall\" (the actor). High mutual attention between subject/verb/object is typical.",
    "successfully": "\"successfully\" is an adverb modifying \"blocked\" ‚Äî it attends most to \"blocked\" (what it modifies) and \"attack\" (the outcome). Adverbs typically attend strongly to their verb and indirectly to the event they're describing."
  };

  // Classify a score into high/mid/low attention tier
  function getHeatClass(score) {
    if (score >= 0.50) return 'heat-high';
    if (score >= 0.25) return 'heat-mid';
    return 'heat-low';
  }

  // Build the demo word chips on page load
  function buildAttentionDemo() {
    const container = document.getElementById('attn-sentence');
    container.innerHTML = '';

    demoWords.forEach(word => {
      const chip = document.createElement('div');
      chip.className   = 'attn-word';
      chip.textContent = word;

      // Score badge (shows the attention value, hidden until a word is clicked)
      const score = document.createElement('span');
      score.className = 'attn-score';
      score.id        = 'score-' + word;
      chip.appendChild(score);

      // Click handler ‚Äî shows attention heat map for this word
      chip.onclick = () => selectWord(word);
      container.appendChild(chip);
    });
  }

  function selectWord(selectedWord) {
    const chips      = document.querySelectorAll('.attn-word');
    const scores     = attentionData[selectedWord];
    const explanation = document.getElementById('attn-explanation');

    // Update each chip's heat class and score badge
    chips.forEach((chip, i) => {
      const word  = demoWords[i];
      const score = scores[i];

      // Remove all heat classes then add the right one
      chip.classList.remove('selected', 'heat-high', 'heat-mid', 'heat-low');

      if (word === selectedWord) {
        chip.classList.add('selected');
      } else {
        chip.classList.add(getHeatClass(score));
      }

      // Show the score badge
      const badge = document.getElementById('score-' + word);
      if (badge && word !== selectedWord) {
        badge.textContent = score.toFixed(2);
        badge.classList.add('visible');
      } else if (badge) {
        badge.classList.remove('visible');
      }
    });

    // Update the explanation text
    explanation.textContent = attentionExplanations[selectedWord] ||
      'Click any word to see its attention pattern.';
  }

  // Build the demo immediately on page load
  buildAttentionDemo();

  // Toggle the attention panel visibility
  function toggleAttentionDemo() {
    const panel = document.getElementById('attention-panel');
    const btn   = document.getElementById('btn-attn');
    if (panel.classList.contains('visible')) {
      panel.classList.remove('visible');
      btn.textContent = 'üîç Interactive Attention Demo';
    } else {
      panel.classList.add('visible');
      btn.textContent = '‚úñ Close Attention Demo';
      // Scroll into view smoothly
      panel.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
    }
  }

</script>
</body>
</html>
